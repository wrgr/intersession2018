{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data\n",
    "\n",
    "First, we want to grab some graphs and subject covariates from a web-accessible url.  We've given this to you on google drive rather than having you set up aws s3 credentials in the interest of saving time. The original data is hosted at m2g.io\n",
    "\n",
    "Below, you will be getting the following dataset:\n",
    "\n",
    "| Property | Value |\n",
    "|:--------:|:-----:|\n",
    "| Dataset  | SWU4  |\n",
    "| N-Subjects  | 454   |\n",
    "| Scans-per-subjects | 2 |\n",
    "| Atlases | Desikan, CPAC200 |\n",
    "| Desikan Nodes | 70 |\n",
    "| CPAC200 Nodes | 200 |\n",
    "\n",
    "The covariates you have are: `SUBID, SESSION, AGE_AT_SCAN_1, SEX, RESTING_STATE_INSTRUCTION, TIME_OF_DAY, SEASON, SATIETY, LMP`. There are other columns in the `.csv` file (downloaded in the next step) but they are populated with a `#` meaning that the value was not recorded.\n",
    "\n",
    "There are several other atlases available - you can change which one you use \n",
    "Running the cell below will get you the data. **Please note, you only have to run these two cells once!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Graphs + Covariates\n",
    "Run the following cells of code to load the graphs into your computer, as well as the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx==1.9 in /opt/conda/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from networkx==1.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx==1.9 #networkx broke backwards compatibility with these graph files\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SWU4/Talairach\n",
      "Datasets: SWU4 (454)\n",
      "Total Subjects: 454\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataset names\n",
    "dataset_names = 'SWU4'\n",
    "\n",
    "basepath = 'data'\n",
    "\n",
    "# change which atlas you use, here!\n",
    "\n",
    "atlas = 'desikan' # 'desikan' # or 'CPAC200', or 'Talairach'\n",
    "dir_names = basepath + '/' + dataset_names + '/' + atlas\n",
    "#basepath = \"/\"\n",
    "#dir_names = basepath\n",
    "print(dir_names)\n",
    "fs = OrderedDict()\n",
    "fs[dataset_names] = [root + \"/\" + fl for root, dirs, files in os.walk(dir_names)\n",
    "                     for fl in files if fl.endswith(\".gpickle\")]\n",
    "\n",
    "ps = \"data/SWU4/SWU4.csv\"\n",
    "\n",
    "print(\"Datasets: \" + \", \".join([fkey + \" (\" + str(len(fs[fkey])) + \")\"\n",
    "                                for fkey in fs]))\n",
    "print(\"Total Subjects: %d\" % (sum([len(fs[key]) for key in fs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGraphs(filenames, verb=False):\n",
    "    \"\"\"\n",
    "    Given a list of files, returns a dictionary of graphs\n",
    "\n",
    "    Required parameters:\n",
    "        filenames:\n",
    "            - List of filenames for graphs\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Initializes empty dictionary\n",
    "    gstruct = OrderedDict()\n",
    "    for idx, files in enumerate(filenames):\n",
    "        if verb:\n",
    "            print(\"Loading: \" + files)\n",
    "        #  Adds graphs to dictionary with key being filename\n",
    "        fname = os.path.basename(files)\n",
    "        gstruct[fname] = nx.read_gpickle(files)\n",
    "    return gstruct\n",
    "\n",
    "def constructGraphDict(names, fs, verb=False):\n",
    "    \"\"\"\n",
    "    Given a set of files and a directory to put things, loads graphs.\n",
    "\n",
    "    Required parameters:\n",
    "        names:\n",
    "            - List of names of the datasets\n",
    "        fs:\n",
    "            - Dictionary of lists of files in each dataset\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Loads graphs into memory for all datasets\n",
    "    graphs = OrderedDict()\n",
    "    if verb:\n",
    "        print(\"Loading Dataset: \" + names)\n",
    "    # The key for the dictionary of graphs is the dataset name\n",
    "    graphs[names] = loadGraphs(fs[names], verb=verb)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2652842f1f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructGraphDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e2b8ad196628>\u001b[0m in \u001b[0;36mconstructGraphDict\u001b[0;34m(names, fs, verb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Dataset: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# The key for the dictionary of graphs is the dataset name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mgraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadGraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e2b8ad196628>\u001b[0m in \u001b[0;36mloadGraphs\u001b[0;34m(filenames, verb)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#  Adds graphs to dictionary with key being filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mgstruct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-148>\u001b[0m in \u001b[0;36mread_gpickle\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Finally, we call the original function, making sure to close the fobj.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/networkx/readwrite/gpickle.py\u001b[0m in \u001b[0;36mread_gpickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# fixture for nose tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "graphs = constructGraphDict(dataset_names, fs, verb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# This gets age and sex, respecitvely.\n",
    "tmp = csv.reader(open(ps,newline='')) # this is the whole phenotype file\n",
    "pheno = OrderedDict()\n",
    "triple = [[t[0].strip(), t[2], int(t[3] == '2')] for t in tmp\n",
    "          if t[3] != '#' and t[2] != '#'][1:]  # female=1->0, male=2->1\n",
    "\n",
    "for idx, trip in enumerate(triple):\n",
    "    pheno[trip[0]] = trip[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(graphs['SWU4'].keys())\n",
    "g = list(graphs['SWU4'].values())\n",
    "k = list(key[6:11] for key in k)\n",
    "k = k[0::2]\n",
    "g1 = g[0::2]\n",
    "g2 = g[1::2]\n",
    "d = dict(zip(k,g1))\n",
    "\n",
    "#Create vectors of labels\n",
    "age = list()\n",
    "sex = list()\n",
    "\n",
    "for key in k:\n",
    "    sex.append(pheno[key][1])\n",
    "    age.append(pheno[key][0])\n",
    "      \n",
    "        \n",
    "# prints number of discrepencies between data and processed data\n",
    "c = 0\n",
    "for i in range(len(g1)):\n",
    "    if sex[i] != pheno[k[i]][1]:\n",
    "        c += 1\n",
    "print (c)\n",
    "        \n",
    "# should use g1 for now - g2 is the retest data for each subject        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT:  \n",
    "(Code above used to get data in the correct format.  Below is a simple example test string with kind of silly features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Combine features, separate training and test data\n",
    "\n",
    "X = []\n",
    "for i in range(len(g1)):\n",
    "    featvec = []\n",
    "#     print (k[i])\n",
    "    \n",
    "    \n",
    "    matrix = nx.to_numpy_matrix(g1[i], nodelist=sorted(g1[i].nodes())) #this is how you go to a matrix\n",
    "    \n",
    "    \"\"\"cc = nx.closeness_centrality(g1[i])\n",
    "    for j in list(cc.values()):\n",
    "        featvec.append(float(j))\"\"\"\n",
    "    \n",
    "    bc = nx.betweenness_centrality(g1[i])\n",
    "    for j in list(bc.values()):\n",
    "        featvec.append(float(j))\n",
    "        \n",
    "    # Add each edge\n",
    "    for edge in np.ravel(matrix):\n",
    "        featvec.append(edge)\n",
    "    \n",
    "    #featvec.extend(np.ravel(np.log(np.sum(matrix,0) + 1)))\n",
    "    featvec.append(nx.average_clustering(g1[i]))\n",
    "    #featvec.append(np.mean(matrix))\"\"\"\n",
    "    \n",
    "    np.shape(featvec)\n",
    "    X.append(featvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X[0:100]\n",
    "Y_train = sex[0:100]\n",
    "\n",
    "X_test = X[100:200]\n",
    "Y_test = sex[100:200]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print (\"Random Forest: \")\n",
    "accuracy = []\n",
    "\n",
    "for ii in range(50): #performance will change over time\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    acc = (clf.predict(X_test) == Y_test)\n",
    "    #print(acc)\n",
    "    accval = (float(np.sum(acc))/float(len(Y_test)))\n",
    "    accuracy.append(accval)\n",
    "#     print('Accuracy:',accval)\n",
    "    \n",
    "print('Overall Accuracy:',str(np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbclf = GaussianNB()\n",
    "nbclf.fit(X_train, Y_train)\n",
    "acc = (clf.predict(X_test) == Y_test)\n",
    "accval = (float(np.sum(acc))/float(len(Y_test)))\n",
    "accuracy.append(accval)\n",
    "print('Naive Bayes Accuracy:',accval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "females = []\n",
    "males = []\n",
    "allSubjects = []\n",
    "\n",
    "for i in range(len(g1)):\n",
    "    # Females are 0, males are 1\n",
    "    matrix = nx.to_numpy_matrix(g1[i], nodelist=sorted(g1[i].nodes()))\n",
    "    allSubjects.append(matrix)\n",
    "    if (sex[i] == 0):\n",
    "        females.append(matrix)\n",
    "    else:\n",
    "        males.append(matrix)\n",
    "\n",
    "# Take the average along the first axis only\n",
    "matrixA = np.mean(allSubjects, axis = 0);\n",
    "plt.imshow(np.log10(matrixA+1))\n",
    "plt.colorbar()\n",
    "plt.title('Mean Connectome')\n",
    "plt.show()\n",
    "        \n",
    "# Take the average along the first axis only\n",
    "matrixM = np.mean(males, axis = 0);\n",
    "plt.imshow(np.log10(matrixM+1))\n",
    "plt.colorbar()\n",
    "plt.title('Mean Male Connectome')\n",
    "plt.show()\n",
    "\n",
    "matrixF = np.mean(females, axis = 0);\n",
    "plt.imshow(np.log10(matrixF + 1))\n",
    "plt.colorbar()\n",
    "plt.title('Mean Female Connectome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retest\n",
    "# We simply use the total difference between each matrix\n",
    "# Does work!!!\n",
    "for i in range(len(g1)):\n",
    "    m1 = nx.to_numpy_matrix(g1[i], nodelist=sorted(g1[i].nodes()))\n",
    "    m2 = nx.to_numpy_matrix(g2[i], nodelist=sorted(g2[i].nodes()))    \n",
    "    self = np.sum(np.log(np.absolute(np.diff([m1, m2], 0)) + 1))\n",
    "    \n",
    "    # Average of other diffs\n",
    "    other = 0\n",
    "    for j in range(len(g1)):\n",
    "        if (i == j): continue\n",
    "        m3 = nx.to_numpy_matrix(g2[j], nodelist=sorted(g2[j].nodes()))\n",
    "        \n",
    "        other += np.sum(np.log(np.absolute(np.diff([m1, m3], 0)) + 1))\n",
    "    other /= float(len(g1)-1)\n",
    "    print(self, other)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
